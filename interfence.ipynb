{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Testing notebook, show results on real world data","metadata":{}},{"cell_type":"code","source":"# !conda install -y cmake\n!pip install face_recognition","metadata":{"execution":{"iopub.status.busy":"2023-02-03T10:14:44.719379Z","iopub.execute_input":"2023-02-03T10:14:44.719782Z","iopub.status.idle":"2023-02-03T10:14:54.191885Z","shell.execute_reply.started":"2023-02-03T10:14:44.719748Z","shell.execute_reply":"2023-02-03T10:14:54.190668Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Requirement already satisfied: face_recognition in /opt/conda/lib/python3.7/site-packages (1.3.0)\nRequirement already satisfied: face-recognition-models>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from face_recognition) (0.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from face_recognition) (1.21.6)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.7/site-packages (from face_recognition) (8.1.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from face_recognition) (9.1.1)\nRequirement already satisfied: dlib>=19.7 in /opt/conda/lib/python3.7/site-packages (from face_recognition) (19.24.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click>=6.0->face_recognition) (6.0.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click>=6.0->face_recognition) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click>=6.0->face_recognition) (4.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport face_recognition\n\nimport numpy as np\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2023-02-03T10:14:54.195476Z","iopub.execute_input":"2023-02-03T10:14:54.197009Z","iopub.status.idle":"2023-02-03T10:14:54.206098Z","shell.execute_reply.started":"2023-02-03T10:14:54.196975Z","shell.execute_reply":"2023-02-03T10:14:54.205138Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"image_size = (224, 224)\n\ntest_transforms = albu.Compose([\n    albu.Resize(*(np.array(image_size) * 1.25).astype(int)),\n    albu.CenterCrop(*image_size),\n    albu.Normalize(),\n    ToTensorV2()\n])\n\n# of course better to move these definitions into separate file\nclass AgeGenderModel(nn.Module):\n    \"\"\"\n    CNN model with 2 heads and SE-block\n    with multitask model learns faster\n    \"\"\"\n    def __init__(self, encoder, encoder_channels, \n                 age_classes, gender_classes, output_channels=512):\n        super().__init__()\n        \n        # encoder features (resnet50 in my case)\n        # output should be bs x c x h x w\n        self.encoder = encoder\n        \n        # sqeeze-excite\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.downsample = nn.Conv2d(encoder_channels, output_channels, 1)\n        self.bn1 = nn.BatchNorm2d(output_channels)\n        self.nonlin1 = nn.ReLU()\n        \n        self.excite = nn.Conv2d(output_channels, output_channels, 1)\n        self.bn2 = nn.BatchNorm2d(output_channels)\n        self.nonlin2 = nn.ReLU()\n        \n        self.age_head = nn.Conv2d(output_channels, age_classes, 1)\n        self.gender_head = nn.Conv2d(output_channels, gender_classes, 1)\n    \n    def forward(self, x):\n        features = self.encoder(x)\n        features = self.squeeze(features)\n        features = self.downsample(features)\n        features = self.nonlin1(self.bn1(features))\n        \n        weights_logits = self.excite(features)\n        features = features * weights_logits.sigmoid()\n        features = self.nonlin2(self.bn2(features))\n        \n        age_logits = self.age_head(features).view(features.size(0), -1)\n        gender_logits = self.gender_head(features).view(features.size(0), -1)\n        return age_logits, gender_logits\n    \n    def inference(self, x, transform=None):\n        \"\"\"Inference one frame\"\"\"\n        # TODO: move to model constructor\n        age_cats = [\"0-2\", \"3-6\", \"8-13\", \"15-20\", \"22-32\", \"34-43\", \"45-53\", \"55-100\"]\n        gender_cats = [\"f\", \"m\", \"u\"]\n        if transform is not None:\n            x = transform(image=x)['image'].unsqueeze(0).to(\"cuda\")\n        age_logits, gender_logits = self.forward(x)\n        return age_cats[age_logits.squeeze(0).max(0)[1]], gender_cats[gender_logits.squeeze(0).max(0)[1]]\n","metadata":{"execution":{"iopub.status.busy":"2023-02-03T10:14:54.208227Z","iopub.execute_input":"2023-02-03T10:14:54.209027Z","iopub.status.idle":"2023-02-03T10:14:54.224472Z","shell.execute_reply.started":"2023-02-03T10:14:54.208991Z","shell.execute_reply":"2023-02-03T10:14:54.223659Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# !pip install timm\nimport timm\nmobilenet_v3 = timm.create_model('tf_mobilenetv3_large_100', pretrained=False)\nmobilenet_v3_encoder = nn.Sequential(*list(mobilenet_v3.children())[:-4])\nmodel = AgeGenderModel(mobilenet_v3_encoder, 960, age_classes=8, gender_classes=3)\nmodel.load_state_dict(torch.load(\"mobilenetv3_age.pth\", map_location=\"cpu\"))\nmodel.eval()\n_ = model.cuda()","metadata":{"execution":{"iopub.status.busy":"2023-02-03T10:14:54.227355Z","iopub.execute_input":"2023-02-03T10:14:54.227769Z","iopub.status.idle":"2023-02-03T10:14:54.420279Z","shell.execute_reply.started":"2023-02-03T10:14:54.227729Z","shell.execute_reply":"2023-02-03T10:14:54.419310Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"im = cv2.imread(\"/kaggle/input/adiencegender/AdienceGender/aligned/100003415@N08/landmark_aligned_face.2174.9523333835_c7887c3fde_o.jpg\")[..., ::-1]\nim = cv2.resize(im, (0, 0), fx=0.5, fy=0.5)\n\nfor (top, right, bottom, left) in face_recognition.face_locations(im):\n    face_crop = im[top:bottom, left:right].copy()\n    plt.imshow(face_crop)\n    plt.show()\n    print(model.inference(face_crop, transform=test_transforms))","metadata":{"execution":{"iopub.status.busy":"2023-02-03T10:14:54.422005Z","iopub.execute_input":"2023-02-03T10:14:54.422404Z","iopub.status.idle":"2023-02-03T10:14:54.565619Z","shell.execute_reply.started":"2023-02-03T10:14:54.422368Z","shell.execute_reply":"2023-02-03T10:14:54.564625Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"Ok. Quality on real world data is slightly wrong (I checked on more images)\n\nSo actually I am overfitted.. :)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}